---
title: Related Research
description: >-

---

<div style="width:1340px;height:300px;margin:auto;padding-top:60px;">

<div class="image" style="width:300px;float:left;margin-right:40px;">
	<p style="font-size:20px;font-family:Helvetica;margin:0px;margin-bottom:20px;margin-top:10px;text-align:right;">Dec. 1, 2024</p>
	<img src="{{ site.baseurl }}/images/acl.png" alt="Screenshot" class="editable" />
</div>
<div style="width:800px;float:left;">
<h2 style="font-size:30px;text-align:left;">MAGMaR Workshop accepted for co-location at ACL 2025</h2>
<p style="font-size:20px;font-family:Helvetica">We are happy to announce that our workshop, MAGMaR (Multimodal Augmented Generation via MultimodAl Retrieval), has been accepted for co-location at <a href="https://2025.aclweb.org/">ACL 2025</a> in Vienna. 
	The workshop centers on multimodal retrieval and retrieval-augmented generation, including a shared task on multilingual event-based video retrieval using the MultiVENT 2.0 dataset.
	We are excited to share more information about submissions and the shared task soon.</p>
</div>
</div>



<div style="width:1340px;height:300px;margin:auto;padding-top:60px;">

	<div class="image" style="width:300px;float:left;margin-right:40px;">
		<p style="font-size:20px;font-family:Helvetica;margin:0px;margin-bottom:20px;margin-top:10px;text-align:right;">Nov. 15, 2024</p>
		<img src="{{ site.baseurl }}/images/emnlp.png" alt="Screenshot" class="editable" />
	</div>
	<div style="width:800px;float:left;">
	<h2 style="font-size:30px;text-align:left;">MultiVENT-Grounded presented at FuturED 2024</h2>
	<p style="font-size:20px;font-family:Helvetica">Alongside a poster at EMNLP 2024, our video event extraction dataset, MultiVENT-Grounded, 
		was presented in an oral talk at the Workshop on the Future of Event Detection. 
		The workshop focuses on detecting real-world events for applications like emergency response and public health, how this field has developed and should continue to develop, extending it beyond NLP, and real-world applications.
		See the live session recording <a href="https://underline.io/events/470/sessions?searchGroup=all">here</a>, and check out the workshop website <a href="https://future-of-event-detection.github.io/">here</a>.</p>
	</div>
	</div>



<div style="width:1340px;height:300px;margin:auto;padding-top:60px;">

	<div class="image" style="width:300px;float:left;margin-right:40px;">
		<p style="font-size:20px;font-family:Helvetica;margin:0px;margin-bottom:20px;margin-top:10px;text-align:right;">Aug. 9, 2024</p>
		<img src="{{ site.baseurl }}/images/clsp.png" alt="Screenshot" class="editable" />
	</div>
	<div style="width:800px;float:left;">
	<h2 style="font-size:30px;text-align:left;">2024 Summer Workshop on Video-Based Event Retrieval</h2>
	<p style="font-size:20px;font-family:Helvetica"><a href="https://hltcoe.jhu.edu/">The HLTCOE</a> (Human Language Technology Center of Excellence) hosted the <a href="https://hltcoe.jhu.edu/research/scale/scale-2024/">2024 iteration of the SCALE summer research workshop</a>.
		SCALEâ€™24 focused on the retrieval of event-based visual content found in both professional and non-professional videos. Our goals of this workshop are to understand how 
		current state-of-the-art computer vision technologies work for the retrieval of multilingual event-based visual content and explore how different modalities can be helpful for this task.
		This workshop resulted in the MultiVENT 2.0 dataset and benchmark approaches.
	</p>
	</div>
	</div>



	<div style="width:1340px;height:300px;margin:auto;padding-top:60px;">

		<div class="image" style="width:300px;float:left;margin-right:40px;">
			<p style="font-size:20px;font-family:Helvetica;margin:0px;margin-bottom:20px;margin-top:10px;text-align:right;">June 17, 2024</p>
			<img src="{{ site.baseurl }}/images/cvpr.png" alt="Screenshot" class="editable" />
		</div>
		<div style="width:800px;float:left;">
		<h2 style="font-size:30px;text-align:left;">Video Events Survey paper presented at CVPR Workshop</h2>
		<p style="font-size:20px;font-family:Helvetica">A Survey of Video Datasets for Grounded Event Understanding was presented at <a href="https://sites.google.com/view/vdu-cvpr24/">the Video Datasets Understanding workshop</a> co-located with 
			CVPR 2024. The survey paper considers the scope of video datasets that implicitly or explicitly target event understanding in multimodal data, compares how these datasets 
			present events, and explores how they compare to multimodal event extraction tasks introduced in the last few years. This survey paper motivates our MultiVENT dataset, and it can be viewed <a href="https://openaccess.thecvf.com/content/CVPR2024W/VDU/html/Sanders_A_Survey_of_Video_Datasets_for_Grounded_Event_Understanding_CVPRW_2024_paper.html">here</a>.
		</p>
		</div>
		</div>